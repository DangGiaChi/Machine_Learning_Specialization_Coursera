{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  \n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])                                           \n",
    "w_tmp = np.array([1.,1.])\n",
    "b_tmp = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_wb(w, b, x):\n",
    "    if x.ndim == 1:\n",
    "        return sigmoid(np.dot(w, x) + b)\n",
    "    elif x.ndim > 1:\n",
    "        return (sigmoid(np.sum(w * x, axis = 1) + b)).reshape(-1, 1)\n",
    "    \n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26894142],\n",
       "       [0.26894142],\n",
       "       [0.26894142],\n",
       "       [0.62245933],\n",
       "       [0.73105858],\n",
       "       [0.62245933]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_wb(w_tmp, b_tmp, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function (BinaryCrossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We have to reshape our inputs to ensure that concatenation happens as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryCrossentropy(y_pred, y_true):\n",
    "    m = len(y_true)\n",
    "\n",
    "    # Reshaping\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "    return np.sum(-y_true * np.log(y_pred) - (1. - y_true) * np.log(1. - y_pred)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the implementation of `BinaryCrossentropy` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36686678640551745"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryCrossentropy(f_wb(w_tmp, b_tmp, X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(w_in = None, b_in = None, function = f_wb, x = None, y = None):\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    m = len(y)\n",
    "    f = function\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    df_dw = np.sum((f(w, b, x) - y) * x, axis = 0) / m\n",
    "    df_db = np.sum(f(w, b, x) - y) / m\n",
    "\n",
    "    return df_dw, df_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the implementation of `Gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49833339 0.49883943] 0.49861806546328574\n"
     ]
    }
   ],
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "df_dw, df_db = Gradient(w_tmp, b_tmp, f_wb, X_tmp, y_tmp)\n",
    "print(df_dw, df_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradentDescentLogistic(w_in = None, b_in = None, function = f_wb, learning_rate = 0.001, x = None, y = None, iterations = 1000, verbose = 0):\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    f = function\n",
    "    a = learning_rate\n",
    "    loss = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        df_dw, df_dw = Gradient(w, b, f, x, y)\n",
    "        w = w - a * df_dw\n",
    "        b = b - a * df_db\n",
    "        loss.append(BinaryCrossentropy(f(w, b, x), y))\n",
    "        \n",
    "        if verbose == 1:\n",
    "            if i % math.floor(iterations / 10) == 0:\n",
    "                print(f\"Iteration {i}    Loss: \" + str(BinaryCrossentropy(f(w, b, x), y)))\n",
    "\n",
    "    return w, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0    Loss: 0.6934579233405945\n",
      "Iteration 1000    Loss: 0.18572250379741753\n",
      "Iteration 2000    Loss: 0.18584824191206326\n",
      "Iteration 3000    Loss: 0.18584834379336146\n",
      "Iteration 4000    Loss: 0.18584834387548918\n",
      "Iteration 5000    Loss: 0.18584834387558494\n",
      "Iteration 6000    Loss: nan\n",
      "Iteration 7000    Loss: nan\n",
      "Iteration 8000    Loss: nan\n",
      "Iteration 9000    Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/frv2q9z92mn9r_np8s6v84cm0000gn/T/ipykernel_14456/204547223.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-y_true * np.log(y_pred) - (1. - y_true) * np.log(1. - y_pred)) / m\n",
      "/var/folders/c1/frv2q9z92mn9r_np8s6v84cm0000gn/T/ipykernel_14456/204547223.py:8: RuntimeWarning: invalid value encountered in multiply\n",
      "  return np.sum(-y_true * np.log(y_pred) - (1. - y_true) * np.log(1. - y_pred)) / m\n"
     ]
    }
   ],
   "source": [
    "w_tmp = np.zeros_like(X_train[-1])\n",
    "b_tmp = 0.\n",
    "a = 0.1\n",
    "iterations = 10000\n",
    "\n",
    "w_trained, b_trained, loss = GradentDescentLogistic(w_tmp, b_tmp, f_wb, a, X_train, y_train, iterations, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
